{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draftnet bot development\n",
    "This notebook tests the performance of a series of bots from draftsimtools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import draftsimtools as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch device.\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load M19 drafts. \n",
    "Data folder is downloadable from draft-data-files on slack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_drafts = ds.load_drafts(\"../data/subset3000/test.csv\")\n",
    "# raw_drafts = ds.load_drafts(\"../../data/m19_2.csv\") # Previous import."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell isn't runnable without AllSets.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here other folks load card lists, but I grab them from json instead\n",
    "# m19_set = ds.create_set(\"data/m19_rating.tsv\", \"data/m19_land_rating.tsv\")\n",
    "\n",
    "with open('../data/AllSets.json', 'r',encoding='utf-8') as json_data:\n",
    "    mtgJSON = json.load(json_data)\n",
    "jsonSubset = mtgJSON['M19']['cards']\n",
    "thisSet = {card['name'] : card for card in jsonSubset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another (fancier) way to create a list of names + lots of other useful stuff\n",
    "nameList = pd.DataFrame.from_dict(thisSet, orient='index') #, columns=['colors','rarity','type','convertedManaCost'])\n",
    "nameList['Name'] = nameList.index                 # We need names as a column, not an index\n",
    "nameList['index'] = range(len(nameList))\n",
    "nameList = nameList.set_index('index')     # And we need a normal numerical index\n",
    "nameList[1:5]\n",
    "\n",
    "# Process names, then handle weird card names (those with commas)\n",
    "nameList['Name'] = nameList.Name.str.replace(' ', '_')\n",
    "\n",
    "# This utility method searches for \"Name\" column in nameList that have commas\n",
    "nameList, raw_drafts = ds.fix_commas(nameList, raw_drafts) # Returns a tuple, as it updates both\n",
    "\n",
    "# Prints namelist\n",
    "#nameList[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load sets normally instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "m19_set = ds.create_set(\"../data/m19_rating.tsv\", \"../data/m19_land_rating.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m19_set = m19_set.sort_values(\"Name\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing draft: 0.\n"
     ]
    }
   ],
   "source": [
    "# Process the drafts, deconstructing packs (hands) at every turn of every draft\n",
    "drafts = ds.process_drafts(raw_drafts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits into (toy) training and test sets. NOTE: For real training, use all drafts.\n",
    "subset_drafts = drafts[:20] # We are only looking at 1000 with this import.\n",
    "#subset_drafts = drafts[5000:5500]\n",
    "#train, test = train_test_split(subset_drafts, test_size = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Unrecognized card:  Island_2\n",
      "---Unrecognized card:  Plains_4\n",
      "---Unrecognized card:  Plains_3\n",
      "---Unrecognized card:  Swamp_4\n",
      "---Unrecognized card:  Forest_3\n",
      "---Unrecognized card:  Forest_2\n",
      "---Unrecognized card:  Swamp_2\n",
      "---Unrecognized card:  Island_3\n",
      "---Unrecognized card:  Mountain_4\n",
      "---Unrecognized card:  Mountain_3\n",
      "---Unrecognized card:  Island_4\n",
      "---Unrecognized card:  Swamp_1\n",
      "---Unrecognized card:  Mountain_2\n",
      "---Unrecognized card:  Swamp_3\n",
      "---Unrecognized card:  Forest_1\n",
      "---Unrecognized card:  Forest_4\n",
      "---Unrecognized card:  Island_1\n",
      "---Unrecognized card:  Plains_2\n",
      "---Unrecognized card:  Plains_1\n",
      "---Unrecognized card:  Mountain_1\n"
     ]
    }
   ],
   "source": [
    "# This should simply add basic lands. \n",
    "\n",
    "# Make sure all cards are listed in the nameList; update if necessary\n",
    "for iDraft in range(len(subset_drafts)):\n",
    "    draft = subset_drafts[iDraft]\n",
    "    for pack in draft:     \n",
    "        for cardName in pack:\n",
    "            try:\n",
    "                pos = nameList[nameList.Name==cardName].index[0]\n",
    "            except:\n",
    "                print(\"---Unrecognized card: \",cardName) # All unrecognized cards here seem to be foil lands\n",
    "                #  \tcolors \trarity \ttype \tconvertedManaCost \tName\n",
    "                nameList = nameList.append({'colors':[],'rarity':'weird','type':'weird',\n",
    "                                            'convertedManaCost':0,'Name':cardName},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n"
     ]
    }
   ],
   "source": [
    "print(len(nameList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits data into training and testing sets.\n",
    "# This doesn't matter for some bots, but for others it does,\n",
    "# so we want to make sure we evaluate on the same testing\n",
    "# set for every bot. \n",
    "train, test = train_test_split(subset_drafts, test_size = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests the random and the raredrafting bots against each other\n",
    "bot1 = ds.RandomBot()\n",
    "# bot2 = ds.RaredraftBot(nameList) # These bots are not runnable without allsets json.\n",
    "# bot3 = ds.ClassicBot(nameList)\n",
    "bot4 = ds.RandomBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = ds.BotTester(subset_drafts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "draft_num    10.500000\n",
      "pick_num     23.000000\n",
      "RandomBot     0.217778\n",
      "MiscBot       0.227778\n",
      "dtype: float64\n",
      "Wrote correct to: output_files/exact_correct.tsv\n",
      "Wrote fuzzy_correct to: output_files/fuzzy_correct.tsv\n",
      "Wrote rank_error to: output_files/rank_error.tsv\n",
      "Wrote card_acc to: output_files/card_accuracies.tsv\n"
     ]
    }
   ],
   "source": [
    "#tester.evaluate_bots([bot1, bot2, bot3], [\"RandomBot\", \"RaredraftBot\", \"ClassicBot\"])\n",
    "tester.evaluate_bots([bot1, bot4], [\"RandomBot\", \"MiscBot\"])\n",
    "tester.report_evaluations()\n",
    "tester.write_evaluations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define draftnet (required for torch import)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell now included in draftsimtools (without GPU support).\n",
    "def create_le(cardnames):\n",
    "    \"\"\"Create label encoder for cardnames.\"\"\"\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(cardnames)\n",
    "    return le\n",
    "\n",
    "def draft_to_matrix(cur_draft, le, pack_size=15):\n",
    "    \"\"\"Transform draft from cardname list to one hot encoding.\"\"\"\n",
    "    pick_list = [np.append(le.transform(cur_draft[i]), (pack_size-len(x))*[0]) \\\n",
    "                 for i, x in enumerate(cur_draft)]\n",
    "    pick_matrix = np.int16(pick_list, device=device)\n",
    "    return pick_matrix\n",
    "\n",
    "def drafts_to_tensor(drafts, le, pack_size=15):\n",
    "    \"\"\"Create tensor of shape (num_drafts, 45, 15).\"\"\"\n",
    "    pick_tensor_list = [draft_to_matrix(d, le) for d in drafts]\n",
    "    pick_tensor = np.int16(pick_tensor_list, device=device)\n",
    "    return pick_tensor\n",
    "\n",
    "#Drafts dataset class.\n",
    "class DraftDataset(Dataset):\n",
    "    \"\"\"Defines a draft dataset in PyTorch.\"\"\"\n",
    "    \n",
    "    def __init__(self, drafts_tensor, le):\n",
    "        \"\"\"Initialization.\n",
    "        \"\"\"\n",
    "        self.drafts_tensor = drafts_tensor\n",
    "        \n",
    "#Torch imports.        \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "#Implement NN.\n",
    "class DraftNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, set_tensor):\n",
    "        \"\"\"Placeholder NN. Currently does nothing.\n",
    "        \n",
    "        param ss: number of cards in set\n",
    "        param set_tensor: Mxss set tensor describing the set\n",
    "        \"\"\"\n",
    "        super(DraftNet, self).__init__()\n",
    "        \n",
    "        # Load set tensor.\n",
    "        self.set_tensor = set_tensor\n",
    "        self.set_tensor_tranpose = torch.transpose(set_tensor, 0, 1)\n",
    "        self.M, self.ss = self.set_tensor.shape\n",
    "        self.half_ss = self.ss / 2\n",
    "        \n",
    "        # Specify layer sizes. \n",
    "        size_in = self.ss + self.M\n",
    "        #size_in = self.ss\n",
    "        size1 = self.ss\n",
    "        size2 = self.ss\n",
    "        size3 = self.ss\n",
    "        size4 = self.ss\n",
    "        size5 = self.ss\n",
    "        size6 = self.ss\n",
    "        size7 = self.ss\n",
    "        size8 = self.ss\n",
    "        \n",
    "        self.ns = 0.01\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(self.ss + self.M)\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(size_in, size1)\n",
    "        self.bn1 = nn.BatchNorm1d(size1)\n",
    "        self.relu1 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear2 = torch.nn.Linear(size1, size2)\n",
    "        self.bn2 = nn.BatchNorm1d(size2)\n",
    "        self.relu2 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear3 = torch.nn.Linear(size2, size3)\n",
    "        self.bn3 = nn.BatchNorm1d(size3)\n",
    "        self.relu3 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear4 = torch.nn.Linear(size3, size4)\n",
    "        self.relu4 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear5 = torch.nn.Linear(size3, size5)\n",
    "        self.relu5 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear6 = torch.nn.Linear(size3, size6)\n",
    "        self.relu6 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        self.dropout6 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear7 = torch.nn.Linear(size3, size7)\n",
    "        self.relu7 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        self.dropout7 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear8 = torch.nn.Linear(size3, size8)\n",
    "        self.relu8 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        \n",
    "        \n",
    "        #self.sm = torch.nn.Softmax()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        \n",
    "        collection = x[:, :self.ss]\n",
    "        \n",
    "        #collection = self.bn(collection)\n",
    "        \n",
    "        pack = x[:, self.ss:]\n",
    "        \n",
    "        # Get features from set tensor. \n",
    "        features = torch.mm(collection, self.set_tensor_tranpose)\n",
    "        collection_and_features = torch.cat((collection, features), 1)\n",
    "        \n",
    "        collection_and_features = self.bn(collection_and_features)\n",
    "        \n",
    "        #y = self.linear1(collection_and_features)\n",
    "        y = self.linear1(collection_and_features)\n",
    "        y = self.bn1(y)\n",
    "        y = self.relu1(y)\n",
    "        y = self.dropout1(y)\n",
    "        \n",
    "        y = self.linear2(y)\n",
    "        y = self.bn2(y)\n",
    "        y = self.relu2(y)\n",
    "        y = self.dropout2(y)\n",
    "        \n",
    "        y = self.linear3(y)\n",
    "        y = self.bn3(y)\n",
    "        y = self.relu3(y)\n",
    "        y = self.dropout3(y)\n",
    "\n",
    "        y = self.linear4(y)\n",
    "        #y = self.relu4(y)\n",
    "        #y = self.dropout4(y)\n",
    "        \n",
    "        #y = self.linear5(y)\n",
    "        #y = self.relu5(y)\n",
    "        #y = self.dropout5(y)\n",
    "        \n",
    "        #y = self.linear6(y)\n",
    "        #y = self.relu6(y)\n",
    "        #y = self.dropout6(y)\n",
    "        \n",
    "        #y = self.linear7(y)\n",
    "        #y = self.relu7(y)\n",
    "        #y = self.dropout7(y)\n",
    "        \n",
    "        #y = self.linear8(y)\n",
    "        #y = self.relu8(y)\n",
    "        \n",
    "        y = y * pack # Enforce cards in pack only.\n",
    "        \n",
    "        return y\n",
    "        self.le = le\n",
    "        self.cards_in_set = len(self.le.classes_)\n",
    "        self.pack_size = int(self.drafts_tensor.shape[1]/3)\n",
    "        self.draft_size = self.pack_size*3\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return a training example.\n",
    "        \"\"\"\n",
    "        #Grab information on current draft.\n",
    "        pick_num = index % self.draft_size #0-self.pack_size*3-1\n",
    "        draft_num = int((index - pick_num)/self.draft_size)\n",
    "        \n",
    "        #Generate.\n",
    "        x = self.create_new_x(pick_num, draft_num)\n",
    "        y = self.create_new_y(pick_num, draft_num)\n",
    "        return x, y\n",
    "    \n",
    "    def create_new_x(self, pick_num, draft_num):\n",
    "        \"\"\"Generate x, input, as a row vector.\n",
    "        0:n     : collection vector\n",
    "                  x[i]=n -> collection has n copies of card i\n",
    "        n:2n    : pack vector\n",
    "                  0 -> card not in pack\n",
    "                  1 -> card in pack\n",
    "        Efficiency optimization possible. Iterative adds to numpy array.\n",
    "        \"\"\"\n",
    "        #Initialize collection / cards in pack vector.\n",
    "        x = np.zeros([self.cards_in_set * 2], dtype = \"int16\")\n",
    "        \n",
    "        #Fill in collection vector excluding current pick (first half).\n",
    "        for n in self.drafts_tensor[draft_num, :pick_num, 0]:\n",
    "            x[n] += 1\n",
    "            \n",
    "        #Fill in pack vector.\n",
    "        cards_in_pack =  self.pack_size - pick_num%self.pack_size #Cards in current pack.\n",
    "        for n in self.drafts_tensor[draft_num, pick_num, :cards_in_pack]:\n",
    "            x[n + self.cards_in_set] = 1\n",
    "            \n",
    "        #Convert to Torch tensor.\n",
    "        x = torch.Tensor(x)\n",
    "        return x\n",
    "    \n",
    "    def create_new_y(self, pick_num, draft_num, not_in_pack=0.5):\n",
    "        \"\"\"Generate y, a target pick row vector.\n",
    "        Picked card is assigned a value of 1.\n",
    "        Other cards are assigned a value of 0.\n",
    "        \"\"\"\n",
    "        #Initialize target vector.\n",
    "        #y = np.array([0] * self.cards_in_set)\n",
    "        y = np.zeros([self.cards_in_set], dtype = \"int16\")\n",
    "            \n",
    "        #Add picked card.\n",
    "        y[self.drafts_tensor[draft_num, pick_num, 0]] = 1\n",
    "        #y = torch.Tensor(y, dtype=torch.int64) # Needed as target.\n",
    "        y = torch.tensor(y, dtype=torch.int64, device=device) # Needed as target.\n",
    "        return y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.drafts_tensor) * self.draft_size\n",
    "\n",
    "def load_dataset(rating_path1, rating_path2, drafts_path):\n",
    "    \"\"\"Create drafts tensor from drafts and set files.\"\"\"\n",
    "    # Load the set. inputs\n",
    "    cur_set = ds.create_set(rating_path1, rating_path2)\n",
    "    raw_drafts = ds.load_drafts(drafts_path)\n",
    "    \n",
    "    # Fix commas. \n",
    "    cur_set, raw_drafts = ds.fix_commas(cur_set, raw_drafts)\n",
    "    \n",
    "    # Process drafts. \n",
    "    drafts = ds.process_drafts(raw_drafts)\n",
    "    \n",
    "    # Drop empty elements at end, if present. \n",
    "    while len(drafts[-1]) == 0:\n",
    "        drafts = drafts[:-1]\n",
    "    \n",
    "    # Create a label encoder.\n",
    "    le = create_le(cur_set[\"Name\"].values)\n",
    "    \n",
    "    # Create drafts tensor. \n",
    "    drafts_tensor = drafts_to_tensor(drafts, le)\n",
    "    \n",
    "    # Create a dataset.\n",
    "    cur_dataset = DraftDataset(drafts_tensor, le)\n",
    "    \n",
    "    # Get the tensor\n",
    "    return cur_dataset, drafts_tensor, cur_set, le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load network from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DraftNet(\n",
       "  (bn): BatchNorm1d(306, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear1): Linear(in_features=306, out_features=285, bias=True)\n",
       "  (bn1): BatchNorm1d(285, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): LeakyReLU(negative_slope=0.01)\n",
       "  (dropout1): Dropout(p=0.5)\n",
       "  (linear2): Linear(in_features=285, out_features=285, bias=True)\n",
       "  (bn2): BatchNorm1d(285, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): LeakyReLU(negative_slope=0.01)\n",
       "  (dropout2): Dropout(p=0.5)\n",
       "  (linear3): Linear(in_features=285, out_features=285, bias=True)\n",
       "  (bn3): BatchNorm1d(285, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): LeakyReLU(negative_slope=0.01)\n",
       "  (dropout3): Dropout(p=0.5)\n",
       "  (linear4): Linear(in_features=285, out_features=285, bias=True)\n",
       "  (relu4): LeakyReLU(negative_slope=0.01)\n",
       "  (dropout4): Dropout(p=0.5)\n",
       "  (linear5): Linear(in_features=285, out_features=285, bias=True)\n",
       "  (relu5): LeakyReLU(negative_slope=0.01)\n",
       "  (dropout5): Dropout(p=0.5)\n",
       "  (linear6): Linear(in_features=285, out_features=285, bias=True)\n",
       "  (relu6): LeakyReLU(negative_slope=0.01)\n",
       "  (dropout6): Dropout(p=0.5)\n",
       "  (linear7): Linear(in_features=285, out_features=285, bias=True)\n",
       "  (relu7): LeakyReLU(negative_slope=0.01)\n",
       "  (dropout7): Dropout(p=0.5)\n",
       "  (linear8): Linear(in_features=285, out_features=285, bias=True)\n",
       "  (relu8): LeakyReLU(negative_slope=0.01)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_net = torch.load(\"./bots_data/draftnet_oct_17_2019_633_cpu.pt\")\n",
    "test_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing draft: 0.\n"
     ]
    }
   ],
   "source": [
    "# Define rating file paths. \n",
    "rating_path1 = \"../data/m19_rating.tsv\"\n",
    "rating_path2 = \"../data/m19_land_rating.tsv\"\n",
    "drafts_path = \"../data/subset3000/train.csv\"\n",
    "\n",
    "# Load data. \n",
    "train_data, train_tensor, m19_set, le = ds.load_dataset(rating_path1, rating_path2, drafts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Collection, pack -> x\n",
    "\n",
    "# Note: draft_frame = [collection, pack]\n",
    "\n",
    "\n",
    "# 2. Draftnet on x\n",
    "# 3. Return ranked output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draft frame only exists internall in the bot_tester class.\n",
    "# Pragmatically speaking, make up a fake collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collection_pack_to_x(collection, pack, le):\n",
    "    \"\"\"Generate x, input, as a row vector.\n",
    "    0:n     : collection vector\n",
    "              x[i]=n -> collection has n copies of card i\n",
    "    n:2n    : pack vector\n",
    "              0 -> card not in pack\n",
    "              1 -> card in pack\n",
    "              \n",
    "    :param collection: cardnames in collection list[string]\n",
    "    :param pack: cardnames in pack list[string]\n",
    "    :param le: label encoder\n",
    "    \n",
    "    :return: x vector\n",
    "    \"\"\"\n",
    "    \n",
    "    #Initialize collection / cards in pack vector.\n",
    "    cards_in_set = len(le.classes_)    \n",
    "    x = np.zeros([cards_in_set * 2], dtype = \"int16\")\n",
    "\n",
    "    #Fill in collection vector.\n",
    "    collection_indices = le.transform(collection)\n",
    "    for ci in collection_indices:\n",
    "        x[ci] += 1\n",
    "\n",
    "    #Fill in pack vector.\n",
    "    pack_indices = le.transform(pack)\n",
    "    for pi in pack_indices:\n",
    "        x[pi + cards_in_set] += 1\n",
    "\n",
    "    #Convert to Torch tensor.\n",
    "    x = torch.Tensor(x).reshape(1, -1) # Include batch dimension.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on an example pack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Knightly_Valor',\n",
       " 'Exclusion_Mage',\n",
       " 'Dwindle',\n",
       " 'Rise_from_the_Grave',\n",
       " 'Daggerback_Basilisk',\n",
       " \"Knight's_Pledge\",\n",
       " 'Skeleton_Archer',\n",
       " 'Epicure_of_Blood',\n",
       " 'Anticipate',\n",
       " 'Cancel',\n",
       " 'Hired_Blade',\n",
       " 'Hostile_Minotaur',\n",
       " 'Crash_Through',\n",
       " 'Plains_4']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create demo collection. \n",
    "demo_collection = tester.drafts[0][0]\n",
    "demo_pack = tester.drafts[0][1]\n",
    "demo_x = collection_pack_to_x(demo_collection, demo_pack, le)\n",
    "\n",
    "# Return the result. \n",
    "result = test_net(demo_x)\n",
    "\n",
    "# Compute the ranked result.\n",
    "pack_tuples = [(i, float(v.detach().numpy())) for i, v in enumerate(result[0, :]) if v > 0]\n",
    "pack_tuples.sort(key = lambda t: t[1], reverse=True)\n",
    "ranked_card_names = [le.inverse_transform(t[0]) for t in pack_tuples]\n",
    "\n",
    "# Display the result. \n",
    "display(ranked_card_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do: implement in bot class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
