{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitbaseconda0082ed13fee24bc88fcf1ac896adab71",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CV dataset\n",
    "\n",
    "This notebook creates a dataset that enables 3-fold cross-validation for the NNet bot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import draftsimtools as ds\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial dataset\n",
    "Load data from original M19 draftsim data files and process it. Some of this processing is unnecessary since it was already performed, but better safe than sorry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Processing draft: 0.\nProcessing draft: 10000.\nProcessing draft: 20000.\nProcessing draft: 30000.\nProcessing draft: 40000.\nProcessing draft: 50000.\nProcessing draft: 60000.\nProcessing draft: 70000.\nProcessing draft: 80000.\n"
    }
   ],
   "source": [
    "dataset_path = \"../../data/\"\n",
    "rating_path1 = dataset_path + \"m19_rating.tsv\"\n",
    "rating_path2 = dataset_path + \"m19_land_rating.tsv\"\n",
    "drafts_path = dataset_path + \"full_dataset/train.csv\"\n",
    "cur_set = ds.create_set(rating_path1, rating_path2)\n",
    "raw_drafts = ds.load_drafts(drafts_path)\n",
    "cur_set, raw_drafts = ds.fix_commas(cur_set, raw_drafts)\n",
    "le = ds.create_le(cur_set[\"Name\"].values)\n",
    "drafts = ds.process_drafts(raw_drafts)\n",
    "drafts = [d for d in drafts if len(d)==45] # Remove incomplete drafts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data\n",
    "\n",
    "Splits data for 3-fold cross-validation and saves to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separates the training data into thirds\n",
    "third = int(len(drafts) / 3)\n",
    "fold1 = drafts[0:third]\n",
    "fold2 = drafts[third:(third*2)]\n",
    "fold3 = drafts[(third*2):(len(drafts) + 1)]\n",
    "\n",
    "# Gets CV splits and converts to tensor\n",
    "split1_train = ds.drafts_to_tensor(fold1 + fold2, le)\n",
    "split1_val = ds.drafts_to_tensor(fold3, le)\n",
    "split2_train = ds.drafts_to_tensor(fold1 + fold3, le)\n",
    "split2_val = ds.drafts_to_tensor(fold2, le)\n",
    "split3_train = ds.drafts_to_tensor(fold2 + fold3, le)\n",
    "split3_val = ds.drafts_to_tensor(fold1, le)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for pickling data\n",
    "def serialize_data(obj, path):\n",
    "    \"\"\"\n",
    "    Serialize an object as a python pickle file.\n",
    "    \"\"\"\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "# Writes all splits to file\n",
    "output_folder = \"bots_data/nnet_train/\"\n",
    "serialize_data(split1_train, output_folder + \"split1_train.pkl\")\n",
    "serialize_data(split1_val, output_folder + \"split1_val.pkl\")\n",
    "serialize_data(split2_train, output_folder + \"split2_train.pkl\")\n",
    "serialize_data(split2_val, output_folder + \"split2_val.pkl\")\n",
    "serialize_data(split3_train, output_folder + \"split3_train.pkl\")\n",
    "serialize_data(split3_val, output_folder + \"split3_val.pkl\")"
   ]
  }
 ]
}