{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DraftNet Development\n",
    "daniel.brooks@alumni.caltech.edu <br>\n",
    "July 1, 2019 <br>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing imports.\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "import draftsimtools as ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torch imports.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle GPU/CPU mode.\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell now included in draftsimtools (without GPU support).\n",
    "def create_le(cardnames):\n",
    "    \"\"\"Create label encoder for cardnames.\"\"\"\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(cardnames)\n",
    "    return le\n",
    "\n",
    "def draft_to_matrix(cur_draft, le, pack_size=15):\n",
    "    \"\"\"Transform draft from cardname list to one hot encoding.\"\"\"\n",
    "    pick_list = [np.append(le.transform(cur_draft[i]), (pack_size-len(x))*[0]) \\\n",
    "                 for i, x in enumerate(cur_draft)]\n",
    "    pick_matrix = np.int16(pick_list, device=device)\n",
    "    return pick_matrix\n",
    "\n",
    "def drafts_to_tensor(drafts, le, pack_size=15):\n",
    "    \"\"\"Create tensor of shape (num_drafts, 45, 15).\"\"\"\n",
    "    pick_tensor_list = [draft_to_matrix(d, le) for d in drafts]\n",
    "    pick_tensor = np.int16(pick_tensor_list, device=device)\n",
    "    return pick_tensor\n",
    "\n",
    "#Drafts dataset class.\n",
    "class DraftDataset(Dataset):\n",
    "    \"\"\"Defines a draft dataset in PyTorch.\"\"\"\n",
    "    \n",
    "    def __init__(self, drafts_tensor, le):\n",
    "        \"\"\"Initialization.\n",
    "        \"\"\"\n",
    "        self.drafts_tensor = drafts_tensor\n",
    "        self.le = le\n",
    "        self.cards_in_set = len(self.le.classes_)\n",
    "        self.pack_size = int(self.drafts_tensor.shape[1]/3)\n",
    "        self.draft_size = self.pack_size*3\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return a training example.\n",
    "        \"\"\"\n",
    "        #Grab information on current draft.\n",
    "        pick_num = index % self.draft_size #0-self.pack_size*3-1\n",
    "        draft_num = int((index - pick_num)/self.draft_size)\n",
    "        \n",
    "        #Generate.\n",
    "        x = self.create_new_x(pick_num, draft_num)\n",
    "        y = self.create_new_y(pick_num, draft_num)\n",
    "        return x, y\n",
    "    \n",
    "    def create_new_x(self, pick_num, draft_num):\n",
    "        \"\"\"Generate x, input, as a row vector.\n",
    "        0:n     : collection vector\n",
    "                  x[i]=n -> collection has n copies of card i\n",
    "        n:2n    : pack vector\n",
    "                  0 -> card not in pack\n",
    "                  1 -> card in pack\n",
    "        Efficiency optimization possible. Iterative adds to numpy array.\n",
    "        \"\"\"\n",
    "        #Initialize collection / cards in pack vector.\n",
    "        x = np.zeros([self.cards_in_set * 2], dtype = \"int16\")\n",
    "        \n",
    "        #Fill in collection vector excluding current pick (first half).\n",
    "        for n in self.drafts_tensor[draft_num, :pick_num, 0]:\n",
    "            x[n] += 1\n",
    "            \n",
    "        #Fill in pack vector.\n",
    "        cards_in_pack =  self.pack_size - pick_num%self.pack_size #Cards in current pack.\n",
    "        for n in self.drafts_tensor[draft_num, pick_num, :cards_in_pack]:\n",
    "            x[n + self.cards_in_set] = 1\n",
    "            \n",
    "        #Convert to Torch tensor.\n",
    "        x = torch.Tensor(x)\n",
    "        return x\n",
    "    \n",
    "    def create_new_y(self, pick_num, draft_num, not_in_pack=0.5):\n",
    "        \"\"\"Generate y, a target pick row vector.\n",
    "        Picked card is assigned a value of 1.\n",
    "        Other cards are assigned a value of 0.\n",
    "        \"\"\"\n",
    "        #Initialize target vector.\n",
    "        #y = np.array([0] * self.cards_in_set)\n",
    "        y = np.zeros([self.cards_in_set], dtype = \"int16\")\n",
    "            \n",
    "        #Add picked card.\n",
    "        y[self.drafts_tensor[draft_num, pick_num, 0]] = 1\n",
    "        #y = torch.Tensor(y, dtype=torch.int64) # Needed as target.\n",
    "        y = torch.tensor(y, dtype=torch.int64, device=device) # Needed as target.\n",
    "        return y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.drafts_tensor) * self.draft_size\n",
    "\n",
    "def load_dataset(rating_path1, rating_path2, drafts_path):\n",
    "    \"\"\"Create drafts tensor from drafts and set files.\"\"\"\n",
    "    # Load the set. inputs\n",
    "    cur_set = ds.create_set(rating_path1, rating_path2)\n",
    "    raw_drafts = ds.load_drafts(drafts_path)\n",
    "    \n",
    "    # Fix commas. \n",
    "    cur_set, raw_drafts = ds.fix_commas(cur_set, raw_drafts)\n",
    "    \n",
    "    # Process drafts. \n",
    "    drafts = ds.process_drafts(raw_drafts)\n",
    "    \n",
    "    # Drop empty elements at end, if present. \n",
    "    while len(drafts[-1]) == 0:\n",
    "        drafts = drafts[:-1]\n",
    "    \n",
    "    # Create a label encoder.\n",
    "    le = create_le(cur_set[\"Name\"].values)\n",
    "    \n",
    "    # Create drafts tensor. \n",
    "    drafts_tensor = drafts_to_tensor(drafts, le)\n",
    "    \n",
    "    # Create a dataset.\n",
    "    cur_dataset = DraftDataset(drafts_tensor, le)\n",
    "    \n",
    "    # Get the tensor\n",
    "    return cur_dataset, drafts_tensor, cur_set, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing draft: 0.\n",
      "Processing draft: 0.\n"
     ]
    }
   ],
   "source": [
    "# Define rating file paths. \n",
    "rating_path1 = \"data/m19_rating.tsv\"\n",
    "rating_path2 = \"data/m19_land_rating.tsv\"\n",
    "\n",
    "# Load data. \n",
    "train_data, train_tensor, m19_set, le = ds.load_dataset(rating_path1, rating_path2, \"data/subset3000/train.csv\")\n",
    "val_data, val_tensor, m19_set, le = ds.load_dataset(rating_path1, rating_path2, \"data/subset3000/val.csv\")\n",
    "#test_data, test_tensor, m19_set, le = load_dataset(rating_path1, rating_path2, \"data/subset20000/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set_vector(casting_cost, card_type, rarity, color_vector):\n",
    "    \"\"\"\n",
    "    Returns a one hot encoded card property vector. \n",
    "    \n",
    "    There are 21 binary features:\n",
    "    \n",
    "    0. cmc=0\n",
    "    1. cmc=1\n",
    "    2. cmc=2\n",
    "    3. cmc=3\n",
    "    4. cmc=4\n",
    "    5. cmc=5\n",
    "    6. cmc=6\n",
    "    7. cmc>=7\n",
    "    8. creature?\n",
    "    9. common?\n",
    "    10. uncommon?\n",
    "    11. rare?\n",
    "    12. mythic?\n",
    "    13. colorless?\n",
    "    14. monocolored?\n",
    "    15. multicolored?\n",
    "    16. color1?\n",
    "    17. color2?\n",
    "    18. color3?\n",
    "    19. color4?\n",
    "    20. color5?\n",
    "    \n",
    "    :param casting_cost: integer casting cost of card\n",
    "    :param card_type: \"Creature\" or other\n",
    "    :param rarity\": \"C\", \"U\", \"R\", or \"M\"\n",
    "    \"param color_vector\": vector corresponding to colors of card, example: [1,0,0,0,1]\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialize set vector.\n",
    "    v = [0] * 21\n",
    "    \n",
    "    # Encode cmc. \n",
    "    if casting_cost == 0:\n",
    "        v[0] = 1\n",
    "    elif casting_cost == 1:\n",
    "        v[1] = 1\n",
    "    elif casting_cost == 2:\n",
    "        v[2] = 1\n",
    "    elif casting_cost == 3:\n",
    "        v[3] = 1\n",
    "    elif casting_cost == 4:\n",
    "        v[4] = 1\n",
    "    elif casting_cost == 5:\n",
    "        v[5] = 1\n",
    "    elif casting_cost == 6:\n",
    "        v[6] = 1\n",
    "    elif casting_cost >= 7:\n",
    "        v[7] = 1\n",
    "    else:\n",
    "        print(\"WARNING: Undefined casting cost.\")\n",
    "    \n",
    "    # Encode type.\n",
    "    if card_type == \"Creature\":\n",
    "        v[8] = 1\n",
    "        \n",
    "    # Encode rarity.\n",
    "    if rarity == \"C\":\n",
    "        v[9] = 1\n",
    "    elif rarity == \"U\":\n",
    "        v[10] = 1\n",
    "    elif rarity == \"R\":\n",
    "        v[11] = 1\n",
    "    elif rarity == \"M\":\n",
    "        v[12] = 1\n",
    "    \n",
    "    # Process number of colors.\n",
    "    num_colors = len([c for c in color_vector if c > 0])\n",
    "    if num_colors == 0:\n",
    "        v[13] = 1\n",
    "    elif num_colors == 1:\n",
    "        v[14] = 1\n",
    "    elif num_colors >= 2:\n",
    "        v[15] = 1\n",
    "    \n",
    "    # Process card color. \n",
    "    if color_vector[0] > 0:\n",
    "        v[16] = 1\n",
    "    if color_vector[1] > 0:\n",
    "        v[17] = 1\n",
    "    if color_vector[2] > 0:\n",
    "        v[18] = 1\n",
    "    if color_vector[3] > 0:\n",
    "        v[19] = 1\n",
    "    if color_vector[4] > 0:\n",
    "        v[20] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmc_from_string(cmc_string):\n",
    "    \"\"\"\n",
    "    Return an integer converted mana cost from cmc_string. \n",
    "    \n",
    "    Each character adds 1 to cmc. \n",
    "    \n",
    "    :param cmc_string: String or integer representation of cmc. Example: \"1UBR\".\n",
    "    :returns: Integer cmc. Example: 4.\n",
    "    \"\"\"\n",
    "    # If int, we are done. \n",
    "    if type(cmc_string) is int:\n",
    "        return cmc_string\n",
    "    \n",
    "    # Convert string to integer cmc.\n",
    "    cmc = 0\n",
    "    digit_string = \"\"\n",
    "    letters = set(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "    digits = set(\"1234567890\")\n",
    "        \n",
    "    for c in cmc_string:        \n",
    "        if c in letters:\n",
    "            cmc += 1\n",
    "        else:\n",
    "            digit_string += c\n",
    "    if len(digit_string) > 0:\n",
    "        cmc += int(digit_string)\n",
    "    return cmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set_tensor(magic_set):\n",
    "    \"\"\"\n",
    "    Returns a set tensor which represents the properties of cards in the set.\n",
    "    \n",
    "    There are M features and N cards in the set and the tensor is of size M x N.\n",
    "    \n",
    "    The features are documented in the create_set_vector() function. \n",
    "    \"\"\"\n",
    "    set_list = []\n",
    "    \n",
    "    # Requires these names to be present in the set file.\n",
    "    reduced_set = magic_set[[\"Name\", \"Casting Cost 1\", \"Card Type\", \"Rarity\", \"Color Vector\"]]\n",
    "    for index, row in reduced_set.iterrows():\n",
    "        card_vector = create_set_vector(cmc_from_string(row[1]), row[2], row[3], row[4])\n",
    "        set_list.append(card_vector)\n",
    "        \n",
    "    # set_list is currently N x M list of lists. \n",
    "    set_flipped = torch.Tensor(set_list)\n",
    "    set_tensor = torch.transpose(set_flipped, 0, 1)\n",
    "    return set_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21, 285])\n"
     ]
    }
   ],
   "source": [
    "# Set tensor.\n",
    "st = create_set_tensor(m19_set).cuda()\n",
    "print(st.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement NN.\n",
    "class DraftNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, set_tensor):\n",
    "        \"\"\"Placeholder NN. Currently does nothing.\n",
    "        \n",
    "        param ss: number of cards in set\n",
    "        param set_tensor: Mxss set tensor describing the set\n",
    "        \"\"\"\n",
    "        super(DraftNet, self).__init__()\n",
    "        \n",
    "        # Load set tensor.\n",
    "        self.set_tensor = set_tensor\n",
    "        self.set_tensor_tranpose = torch.transpose(set_tensor, 0, 1)\n",
    "        self.M, self.ss = self.set_tensor.shape\n",
    "        self.half_ss = self.ss / 2\n",
    "        \n",
    "        # Specify layer sizes. \n",
    "        size_in = self.ss + self.M\n",
    "        #size_in = self.ss\n",
    "        size1 = self.ss\n",
    "        size2 = self.ss\n",
    "        size3 = self.ss\n",
    "        size4 = self.ss\n",
    "        size5 = self.ss\n",
    "        size6 = self.ss\n",
    "        size7 = self.ss\n",
    "        size8 = self.ss\n",
    "        \n",
    "        self.ns = 0.01\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(self.ss + self.M)\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(size_in, size1)\n",
    "        self.bn1 = nn.BatchNorm1d(size1)\n",
    "        self.relu1 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear2 = torch.nn.Linear(size1, size2)\n",
    "        self.bn2 = nn.BatchNorm1d(size2)\n",
    "        self.relu2 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear3 = torch.nn.Linear(size2, size3)\n",
    "        self.bn3 = nn.BatchNorm1d(size3)\n",
    "        self.relu3 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear4 = torch.nn.Linear(size3, size4)\n",
    "        self.relu4 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear5 = torch.nn.Linear(size3, size5)\n",
    "        self.relu5 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear6 = torch.nn.Linear(size3, size6)\n",
    "        self.relu6 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        self.dropout6 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear7 = torch.nn.Linear(size3, size7)\n",
    "        self.relu7 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        self.dropout7 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear8 = torch.nn.Linear(size3, size8)\n",
    "        self.relu8 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        \n",
    "        \n",
    "        #self.sm = torch.nn.Softmax()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        \n",
    "        collection = x[:, :self.ss]\n",
    "        \n",
    "        #collection = self.bn(collection)\n",
    "        \n",
    "        pack = x[:, self.ss:]\n",
    "        \n",
    "        # Get features from set tensor. \n",
    "        features = torch.mm(collection, self.set_tensor_tranpose)\n",
    "        collection_and_features = torch.cat((collection, features), 1)\n",
    "        \n",
    "        collection_and_features = self.bn(collection_and_features)\n",
    "        \n",
    "        #y = self.linear1(collection_and_features)\n",
    "        y = self.linear1(collection_and_features)\n",
    "        y = self.bn1(y)\n",
    "        y = self.relu1(y)\n",
    "        y = self.dropout1(y)\n",
    "        \n",
    "        y = self.linear2(y)\n",
    "        y = self.bn2(y)\n",
    "        y = self.relu2(y)\n",
    "        y = self.dropout2(y)\n",
    "        \n",
    "        y = self.linear3(y)\n",
    "        y = self.bn3(y)\n",
    "        y = self.relu3(y)\n",
    "        y = self.dropout3(y)\n",
    "\n",
    "        y = self.linear4(y)\n",
    "        #y = self.relu4(y)\n",
    "        #y = self.dropout4(y)\n",
    "        \n",
    "        #y = self.linear5(y)\n",
    "        #y = self.relu5(y)\n",
    "        #y = self.dropout5(y)\n",
    "        \n",
    "        #y = self.linear6(y)\n",
    "        #y = self.relu6(y)\n",
    "        #y = self.dropout6(y)\n",
    "        \n",
    "        #y = self.linear7(y)\n",
    "        #y = self.relu7(y)\n",
    "        #y = self.dropout7(y)\n",
    "        \n",
    "        #y = self.linear8(y)\n",
    "        #y = self.relu8(y)\n",
    "        \n",
    "        y = y * pack # Enforce cards in pack only.\n",
    "        \n",
    "        return y\n",
    "\n",
    "#Create NN.\n",
    "net = DraftNet(st).cuda()\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net, dataloader, num_epoch, optimizer):\n",
    "    \"\"\"Train the network.\"\"\"\n",
    "    net.train()    \n",
    "    my_count = 0\n",
    "    for epoch in range(num_epoch):\n",
    "        \n",
    "        #Loop over x,y for each dataset.\n",
    "        running_loss = 0\n",
    "        for i, data in enumerate(dataloader):\n",
    "        \n",
    "            my_count+=1\n",
    "            if my_count % 10000 == 0:\n",
    "                print(my_count)\n",
    "        \n",
    "            #Get the inputs. Keeps batch size.\n",
    "            x, y = data\n",
    "            \n",
    "            # cuda() is needed for GPU mode. Not sure why.\n",
    "            x = x.cuda()\n",
    "            y = y.cuda() # One-hot encoded. \n",
    "            \n",
    "            # Zero parameter gradients between batches.\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            #Perform training.\n",
    "            y_pred = net(x)\n",
    "            y_integer = torch.argmax(y, 1) # Class indices.\n",
    "            \n",
    "            # Use cross entropy loss. \n",
    "            loss = torch.nn.CrossEntropyLoss()\n",
    "            output = loss(y_pred, y_integer)\n",
    "            output.backward()\n",
    "            optimizer.step()\n",
    "                        \n",
    "            #Print loss data.\n",
    "            running_loss += output.item()\n",
    "            step = 1\n",
    "            if i % len(dataloader) == len(dataloader)-1 and (epoch + 1) % step == 0:\n",
    "                print('Train Cross-Entropy Loss: %.6f' % (running_loss/len(dataloader)))\n",
    "                running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataloaders. \n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(val_data, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_net(net, dataloader):\n",
    "    \"\"\"Compute accuracy on validation set.\"\"\"\n",
    "    net.eval()\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "        \n",
    "            #Get the inputs. Keeps batch size.\n",
    "            x, y = data\n",
    "            \n",
    "            # cuda() is needed for GPU mode. Not sure why.\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            y_integer = torch.argmax(y, 1) # Class indices.\n",
    "            \n",
    "            # Compute val loss.\n",
    "            y_pred = net(x)\n",
    "            y_pred_integer = torch.argmax(y_pred, 1)\n",
    "            \n",
    "            # Compute accuracy. \n",
    "            correct += int(sum(y_pred_integer == y_integer))\n",
    "            total += len(y_integer)\n",
    "            \n",
    "    accuracy = correct / total\n",
    "\n",
    "    print(\"Validation accuracy:\", accuracy, \" Total picks:\", int(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Cross-Entropy Loss: 3.712706\n",
      "Validation accuracy: 0.43966666666666665  Total picks: 45000\n",
      "Epoch: 2\n",
      "Train Cross-Entropy Loss: 1.872636\n",
      "Validation accuracy: 0.5266444444444445  Total picks: 45000\n",
      "Epoch: 3\n",
      "Train Cross-Entropy Loss: 1.459694\n",
      "Validation accuracy: 0.5560666666666667  Total picks: 45000\n",
      "Epoch: 4\n",
      "Train Cross-Entropy Loss: 1.299285\n",
      "Validation accuracy: 0.5742444444444444  Total picks: 45000\n",
      "Epoch: 5\n",
      "Train Cross-Entropy Loss: 1.220708\n",
      "Validation accuracy: 0.5841777777777778  Total picks: 45000\n",
      "Epoch: 6\n",
      "Train Cross-Entropy Loss: 1.184621\n",
      "Validation accuracy: 0.5904222222222222  Total picks: 45000\n",
      "Epoch: 7\n",
      "Train Cross-Entropy Loss: 1.155239\n",
      "Validation accuracy: 0.5957333333333333  Total picks: 45000\n",
      "Epoch: 8\n",
      "Train Cross-Entropy Loss: 1.136483\n",
      "Validation accuracy: 0.5918222222222222  Total picks: 45000\n",
      "Epoch: 9\n",
      "Train Cross-Entropy Loss: 1.118754\n",
      "Validation accuracy: 0.5973555555555555  Total picks: 45000\n",
      "Epoch: 10\n",
      "Train Cross-Entropy Loss: 1.109187\n",
      "Validation accuracy: 0.5994888888888888  Total picks: 45000\n",
      "Epoch: 11\n",
      "Train Cross-Entropy Loss: 1.100882\n",
      "Validation accuracy: 0.6012222222222222  Total picks: 45000\n",
      "Epoch: 12\n",
      "Train Cross-Entropy Loss: 1.088779\n",
      "Validation accuracy: 0.6001333333333333  Total picks: 45000\n",
      "Epoch: 13\n",
      "Train Cross-Entropy Loss: 1.078680\n",
      "Validation accuracy: 0.6008  Total picks: 45000\n",
      "Epoch: 14\n",
      "Train Cross-Entropy Loss: 1.075875\n",
      "Validation accuracy: 0.6016222222222222  Total picks: 45000\n",
      "Epoch: 15\n",
      "Train Cross-Entropy Loss: 1.065087\n",
      "Validation accuracy: 0.6022666666666666  Total picks: 45000\n",
      "Epoch: 16\n",
      "Train Cross-Entropy Loss: 1.059963\n",
      "Validation accuracy: 0.6046222222222222  Total picks: 45000\n",
      "Epoch: 17\n",
      "Train Cross-Entropy Loss: 1.054795\n",
      "Validation accuracy: 0.6025777777777778  Total picks: 45000\n",
      "Epoch: 18\n",
      "Train Cross-Entropy Loss: 1.048514\n",
      "Validation accuracy: 0.6037777777777777  Total picks: 45000\n",
      "Epoch: 19\n",
      "Train Cross-Entropy Loss: 1.037096\n",
      "Validation accuracy: 0.6057111111111111  Total picks: 45000\n",
      "Epoch: 20\n",
      "Train Cross-Entropy Loss: 1.035400\n",
      "Validation accuracy: 0.6076  Total picks: 45000\n",
      "Epoch: 21\n",
      "Train Cross-Entropy Loss: 1.032171\n",
      "Validation accuracy: 0.6068444444444444  Total picks: 45000\n",
      "Epoch: 22\n",
      "Train Cross-Entropy Loss: 1.027813\n",
      "Validation accuracy: 0.6074888888888889  Total picks: 45000\n",
      "Epoch: 23\n",
      "Train Cross-Entropy Loss: 1.019601\n",
      "Validation accuracy: 0.6066  Total picks: 45000\n",
      "Epoch: 24\n",
      "Train Cross-Entropy Loss: 1.015603\n",
      "Validation accuracy: 0.6067333333333333  Total picks: 45000\n",
      "Epoch: 25\n",
      "Train Cross-Entropy Loss: 1.013053\n",
      "Validation accuracy: 0.6061111111111112  Total picks: 45000\n",
      "Epoch: 26\n",
      "Train Cross-Entropy Loss: 1.007975\n",
      "Validation accuracy: 0.6100666666666666  Total picks: 45000\n",
      "Epoch: 27\n",
      "Train Cross-Entropy Loss: 1.004521\n",
      "Validation accuracy: 0.6081777777777778  Total picks: 45000\n",
      "Epoch: 28\n",
      "Train Cross-Entropy Loss: 1.003805\n",
      "Validation accuracy: 0.6086222222222222  Total picks: 45000\n",
      "Epoch: 29\n",
      "Train Cross-Entropy Loss: 0.997265\n",
      "Validation accuracy: 0.6089777777777777  Total picks: 45000\n",
      "Epoch: 30\n",
      "Train Cross-Entropy Loss: 0.992123\n",
      "Validation accuracy: 0.6074  Total picks: 45000\n",
      "Epoch: 31\n",
      "Train Cross-Entropy Loss: 0.993829\n",
      "Validation accuracy: 0.6073777777777778  Total picks: 45000\n",
      "Epoch: 32\n",
      "Train Cross-Entropy Loss: 0.984687\n",
      "Validation accuracy: 0.6084666666666667  Total picks: 45000\n",
      "Epoch: 33\n",
      "Train Cross-Entropy Loss: 0.981102\n",
      "Validation accuracy: 0.6089777777777777  Total picks: 45000\n",
      "Epoch: 34\n",
      "Train Cross-Entropy Loss: 0.980292\n",
      "Validation accuracy: 0.6092666666666666  Total picks: 45000\n",
      "Epoch: 35\n",
      "Train Cross-Entropy Loss: 0.976217\n",
      "Validation accuracy: 0.6097777777777778  Total picks: 45000\n",
      "Epoch: 36\n",
      "Train Cross-Entropy Loss: 0.973579\n",
      "Validation accuracy: 0.6094666666666667  Total picks: 45000\n",
      "Epoch: 37\n",
      "Train Cross-Entropy Loss: 0.967390\n",
      "Validation accuracy: 0.6088888888888889  Total picks: 45000\n",
      "Epoch: 38\n",
      "Train Cross-Entropy Loss: 0.967076\n",
      "Validation accuracy: 0.6106666666666667  Total picks: 45000\n",
      "Epoch: 39\n",
      "Train Cross-Entropy Loss: 0.963652\n",
      "Validation accuracy: 0.6105111111111111  Total picks: 45000\n",
      "Epoch: 40\n",
      "Train Cross-Entropy Loss: 0.964193\n",
      "Validation accuracy: 0.6106666666666667  Total picks: 45000\n",
      "Epoch: 41\n",
      "Train Cross-Entropy Loss: 0.962563\n",
      "Validation accuracy: 0.6101555555555556  Total picks: 45000\n",
      "Epoch: 42\n",
      "Train Cross-Entropy Loss: 0.957154\n",
      "Validation accuracy: 0.6096444444444444  Total picks: 45000\n",
      "Epoch: 43\n",
      "Train Cross-Entropy Loss: 0.955736\n",
      "Validation accuracy: 0.6086666666666667  Total picks: 45000\n",
      "Epoch: 44\n",
      "Train Cross-Entropy Loss: 0.952003\n",
      "Validation accuracy: 0.6116222222222222  Total picks: 45000\n",
      "Epoch: 45\n",
      "Train Cross-Entropy Loss: 0.948907\n",
      "Validation accuracy: 0.6109555555555556  Total picks: 45000\n",
      "Epoch: 46\n",
      "Train Cross-Entropy Loss: 0.949866\n",
      "Validation accuracy: 0.6102222222222222  Total picks: 45000\n",
      "Epoch: 47\n",
      "Train Cross-Entropy Loss: 0.943334\n",
      "Validation accuracy: 0.6087777777777778  Total picks: 45000\n",
      "Epoch: 48\n",
      "Train Cross-Entropy Loss: 0.943766\n",
      "Validation accuracy: 0.6104444444444445  Total picks: 45000\n",
      "Epoch: 49\n",
      "Train Cross-Entropy Loss: 0.936686\n",
      "Validation accuracy: 0.6097777777777778  Total picks: 45000\n",
      "Epoch: 50\n",
      "Train Cross-Entropy Loss: 0.934707\n",
      "Validation accuracy: 0.6098888888888889  Total picks: 45000\n",
      "Epoch: 51\n",
      "Train Cross-Entropy Loss: 0.931474\n",
      "Validation accuracy: 0.6082444444444445  Total picks: 45000\n",
      "Epoch: 52\n",
      "Train Cross-Entropy Loss: 0.933870\n",
      "Validation accuracy: 0.6084666666666667  Total picks: 45000\n",
      "Epoch: 53\n",
      "Train Cross-Entropy Loss: 0.928188\n",
      "Validation accuracy: 0.6096444444444444  Total picks: 45000\n",
      "Epoch: 54\n",
      "Train Cross-Entropy Loss: 0.929221\n",
      "Validation accuracy: 0.6103555555555555  Total picks: 45000\n",
      "Epoch: 55\n",
      "Train Cross-Entropy Loss: 0.928139\n",
      "Validation accuracy: 0.6072666666666666  Total picks: 45000\n",
      "Epoch: 56\n",
      "Train Cross-Entropy Loss: 0.925046\n",
      "Validation accuracy: 0.6105555555555555  Total picks: 45000\n",
      "Epoch: 57\n",
      "Train Cross-Entropy Loss: 0.927743\n",
      "Validation accuracy: 0.6114888888888889  Total picks: 45000\n",
      "Epoch: 58\n",
      "Train Cross-Entropy Loss: 0.917192\n",
      "Validation accuracy: 0.6094222222222222  Total picks: 45000\n",
      "Epoch: 59\n",
      "Train Cross-Entropy Loss: 0.918461\n",
      "Validation accuracy: 0.6095555555555555  Total picks: 45000\n",
      "Epoch: 60\n",
      "Train Cross-Entropy Loss: 0.920125\n",
      "Validation accuracy: 0.6092444444444445  Total picks: 45000\n",
      "Epoch: 61\n",
      "Train Cross-Entropy Loss: 0.914584\n",
      "Validation accuracy: 0.6101111111111112  Total picks: 45000\n",
      "Epoch: 62\n",
      "Train Cross-Entropy Loss: 0.914450\n",
      "Validation accuracy: 0.6093555555555555  Total picks: 45000\n",
      "Epoch: 63\n",
      "Train Cross-Entropy Loss: 0.908125\n",
      "Validation accuracy: 0.6099555555555556  Total picks: 45000\n",
      "Epoch: 64\n",
      "Train Cross-Entropy Loss: 0.911231\n",
      "Validation accuracy: 0.6094444444444445  Total picks: 45000\n",
      "Epoch: 65\n",
      "Train Cross-Entropy Loss: 0.911798\n",
      "Validation accuracy: 0.6100888888888889  Total picks: 45000\n",
      "Epoch: 66\n",
      "Train Cross-Entropy Loss: 0.902200\n",
      "Validation accuracy: 0.6117111111111111  Total picks: 45000\n",
      "Epoch: 67\n",
      "Train Cross-Entropy Loss: 0.902796\n",
      "Validation accuracy: 0.6096222222222222  Total picks: 45000\n",
      "Epoch: 68\n",
      "Train Cross-Entropy Loss: 0.904393\n",
      "Validation accuracy: 0.6097111111111111  Total picks: 45000\n",
      "Epoch: 69\n",
      "Train Cross-Entropy Loss: 0.899434\n",
      "Validation accuracy: 0.6081333333333333  Total picks: 45000\n",
      "Epoch: 70\n",
      "Train Cross-Entropy Loss: 0.902206\n",
      "Validation accuracy: 0.6079111111111111  Total picks: 45000\n",
      "Epoch: 71\n",
      "Train Cross-Entropy Loss: 0.899906\n",
      "Validation accuracy: 0.6087777777777778  Total picks: 45000\n",
      "Epoch: 72\n",
      "Train Cross-Entropy Loss: 0.898986\n",
      "Validation accuracy: 0.6089111111111111  Total picks: 45000\n",
      "Epoch: 73\n",
      "Train Cross-Entropy Loss: 0.892385\n",
      "Validation accuracy: 0.6096222222222222  Total picks: 45000\n",
      "Epoch: 74\n",
      "Train Cross-Entropy Loss: 0.893557\n",
      "Validation accuracy: 0.6080222222222222  Total picks: 45000\n",
      "Epoch: 75\n",
      "Train Cross-Entropy Loss: 0.895254\n",
      "Validation accuracy: 0.6082888888888889  Total picks: 45000\n",
      "Epoch: 76\n",
      "Train Cross-Entropy Loss: 0.893271\n",
      "Validation accuracy: 0.6097111111111111  Total picks: 45000\n",
      "Epoch: 77\n",
      "Train Cross-Entropy Loss: 0.886811\n",
      "Validation accuracy: 0.6081111111111112  Total picks: 45000\n",
      "Epoch: 78\n",
      "Train Cross-Entropy Loss: 0.887924\n",
      "Validation accuracy: 0.6104222222222222  Total picks: 45000\n",
      "Epoch: 79\n",
      "Train Cross-Entropy Loss: 0.892042\n",
      "Validation accuracy: 0.6093333333333333  Total picks: 45000\n",
      "Epoch: 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Cross-Entropy Loss: 0.887499\n",
      "Validation accuracy: 0.6094444444444445  Total picks: 45000\n",
      "Epoch: 81\n",
      "Train Cross-Entropy Loss: 0.883157\n",
      "Validation accuracy: 0.6094222222222222  Total picks: 45000\n",
      "Epoch: 82\n",
      "Train Cross-Entropy Loss: 0.884167\n",
      "Validation accuracy: 0.6085777777777778  Total picks: 45000\n",
      "Epoch: 83\n",
      "Train Cross-Entropy Loss: 0.884888\n",
      "Validation accuracy: 0.6089555555555556  Total picks: 45000\n",
      "Epoch: 84\n",
      "Train Cross-Entropy Loss: 0.880432\n",
      "Validation accuracy: 0.6078222222222223  Total picks: 45000\n",
      "Epoch: 85\n",
      "Train Cross-Entropy Loss: 0.878422\n",
      "Validation accuracy: 0.6093333333333333  Total picks: 45000\n",
      "Epoch: 86\n",
      "Train Cross-Entropy Loss: 0.881337\n",
      "Validation accuracy: 0.6090222222222222  Total picks: 45000\n",
      "Epoch: 87\n",
      "Train Cross-Entropy Loss: 0.878298\n",
      "Validation accuracy: 0.6096444444444444  Total picks: 45000\n",
      "Epoch: 88\n",
      "Train Cross-Entropy Loss: 0.879793\n",
      "Validation accuracy: 0.6082222222222222  Total picks: 45000\n",
      "Epoch: 89\n",
      "Train Cross-Entropy Loss: 0.875361\n",
      "Validation accuracy: 0.6103555555555555  Total picks: 45000\n",
      "Epoch: 90\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e17011f243c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Train1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-6b860402cc92>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(net, dataloader, num_epoch, optimizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m#Loop over x,y for each dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mmy_count\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/keras/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Code/MTG/Dan/draftsimtools/load.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;31m#Generate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_new_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpick_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraft_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_new_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpick_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraft_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Code/MTG/Dan/draftsimtools/load.py\u001b[0m in \u001b[0;36mcreate_new_x\u001b[0;34m(self, pick_num, draft_num)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m#Convert to Torch tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the network over several epochs - no momentu, from scratch.\n",
    "run = 0\n",
    "\n",
    "# Now leave optimizer out here. \n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "#optimizer = optim.ASGD(net.parameters())\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.0)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.33) # Used for exp 47.\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.98)\n",
    "\n",
    "ep = 0\n",
    "\n",
    "for run in range(999999):\n",
    "    \n",
    "    ep += 1\n",
    "    print(\"Epoch:\", ep)\n",
    "    \n",
    "    # Train1.\n",
    "    train_net(net, trainloader, 1, optimizer)\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation. \n",
    "    val_net(net, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.00001, momentum=0.0)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.33) # Used for exp 47.\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.8)\n",
    "\n",
    "ep = 0\n",
    "\n",
    "for run in range(999999):\n",
    "    \n",
    "    ep += 1\n",
    "    print(\"Epoch:\", ep)\n",
    "    \n",
    "    # Train1.\n",
    "    train_net(net, trainloader, 1, optimizer)\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation. \n",
    "    val_net(net, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
