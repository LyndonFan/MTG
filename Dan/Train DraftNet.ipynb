{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DraftNet Development\n",
    "daniel.brooks@alumni.caltech.edu <br>\n",
    "Jan 18, 2020 <br>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing imports.\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "import draftsimtools as ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torch imports.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle GPU/CPU mode.\n",
    "device = torch.device(\"cpu\") # Use CPU device for saving model.\n",
    "#device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell now included in draftsimtools (without GPU support).\n",
    "def create_le(cardnames):\n",
    "    \"\"\"Create label encoder for cardnames.\"\"\"\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(cardnames)\n",
    "    return le\n",
    "\n",
    "def draft_to_matrix(cur_draft, le, pack_size=15):\n",
    "    \"\"\"Transform draft from cardname list to one hot encoding.\"\"\"\n",
    "    pick_list = [np.append(le.transform(cur_draft[i]), (pack_size-len(x))*[0]) \\\n",
    "                 for i, x in enumerate(cur_draft)]\n",
    "    pick_matrix = np.int16(pick_list, device=device)\n",
    "    return pick_matrix\n",
    "\n",
    "def drafts_to_tensor(drafts, le, pack_size=15):\n",
    "    \"\"\"Create tensor of shape (num_drafts, 45, 15).\"\"\"\n",
    "    pick_tensor_list = [draft_to_matrix(d, le) for d in drafts]\n",
    "    pick_tensor = np.int16(pick_tensor_list, device=device)\n",
    "    return pick_tensor\n",
    "\n",
    "#Drafts dataset class.\n",
    "class DraftDataset(Dataset):\n",
    "    \"\"\"Defines a draft dataset in PyTorch.\"\"\"\n",
    "    \n",
    "    def __init__(self, drafts_tensor, le):\n",
    "        \"\"\"Initialization.\n",
    "        \"\"\"\n",
    "        self.drafts_tensor = drafts_tensor\n",
    "        self.le = le\n",
    "        self.cards_in_set = len(self.le.classes_)\n",
    "        self.pack_size = int(self.drafts_tensor.shape[1]/3)\n",
    "        self.draft_size = self.pack_size*3\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return a training example.\n",
    "        \"\"\"\n",
    "        #Grab information on current draft.\n",
    "        pick_num = index % self.draft_size #0-self.pack_size*3-1\n",
    "        draft_num = int((index - pick_num)/self.draft_size)\n",
    "        \n",
    "        #Generate.\n",
    "        x = self.create_new_x(pick_num, draft_num)\n",
    "        y = self.create_new_y(pick_num, draft_num)\n",
    "        return x, y\n",
    "    \n",
    "    def create_new_x(self, pick_num, draft_num):\n",
    "        \"\"\"Generate x, input, as a row vector.\n",
    "        0:n     : collection vector\n",
    "                  x[i]=n -> collection has n copies of card i\n",
    "        n:2n    : pack vector\n",
    "                  0 -> card not in pack\n",
    "                  1 -> card in pack\n",
    "        Efficiency optimization possible. Iterative adds to numpy array.\n",
    "        \"\"\"\n",
    "        #Initialize collection / cards in pack vector.\n",
    "        x = np.zeros([self.cards_in_set * 2], dtype = \"int16\")\n",
    "        \n",
    "        #Fill in collection vector excluding current pick (first half).\n",
    "        for n in self.drafts_tensor[draft_num, :pick_num, 0]:\n",
    "            x[n] += 1\n",
    "            \n",
    "        #Fill in pack vector.\n",
    "        cards_in_pack =  self.pack_size - pick_num%self.pack_size #Cards in current pack.\n",
    "        for n in self.drafts_tensor[draft_num, pick_num, :cards_in_pack]:\n",
    "            x[n + self.cards_in_set] = 1\n",
    "            \n",
    "        #Convert to Torch tensor.\n",
    "        x = torch.Tensor(x)\n",
    "        return x\n",
    "    \n",
    "    def create_new_y(self, pick_num, draft_num, not_in_pack=0.5):\n",
    "        \"\"\"Generate y, a target pick row vector.\n",
    "        Picked card is assigned a value of 1.\n",
    "        Other cards are assigned a value of 0.\n",
    "        \"\"\"\n",
    "        #Initialize target vector.\n",
    "        #y = np.array([0] * self.cards_in_set)\n",
    "        y = np.zeros([self.cards_in_set], dtype = \"int16\")\n",
    "            \n",
    "        #Add picked card.\n",
    "        y[self.drafts_tensor[draft_num, pick_num, 0]] = 1\n",
    "        #y = torch.Tensor(y, dtype=torch.int64) # Needed as target.\n",
    "        y = torch.tensor(y, dtype=torch.int64, device=device) # Needed as target.\n",
    "        return y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.drafts_tensor) * self.draft_size\n",
    "\n",
    "def load_dataset(rating_path1, rating_path2, drafts_path):\n",
    "    \"\"\"Create drafts tensor from drafts and set files.\"\"\"\n",
    "    # Load the set. inputs\n",
    "    cur_set = ds.create_set(rating_path1, rating_path2)\n",
    "    raw_drafts = ds.load_drafts(drafts_path)\n",
    "    \n",
    "    # Fix commas. \n",
    "    cur_set, raw_drafts = ds.fix_commas(cur_set, raw_drafts)\n",
    "    \n",
    "    # Process drafts. \n",
    "    drafts = ds.process_drafts(raw_drafts)\n",
    "    \n",
    "    # Drop empty elements at end, if present. \n",
    "    while len(drafts[-1]) == 0:\n",
    "        drafts = drafts[:-1]\n",
    "    \n",
    "    # Create a label encoder.\n",
    "    le = create_le(cur_set[\"Name\"].values)\n",
    "    \n",
    "    # Create drafts tensor. \n",
    "    drafts_tensor = drafts_to_tensor(drafts, le)\n",
    "    \n",
    "    # Create a dataset.\n",
    "    cur_dataset = DraftDataset(drafts_tensor, le)\n",
    "    \n",
    "    # Get the tensor\n",
    "    return cur_dataset, drafts_tensor, cur_set, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing draft: 0.\n",
      "Processing draft: 10000.\n",
      "Processing draft: 20000.\n",
      "Processing draft: 30000.\n",
      "Processing draft: 40000.\n",
      "Processing draft: 50000.\n",
      "Processing draft: 60000.\n",
      "Processing draft: 70000.\n",
      "Processing draft: 80000.\n"
     ]
    }
   ],
   "source": [
    "# Define rating file paths. \n",
    "rating_path1 = \"data/m19_rating.tsv\"\n",
    "rating_path2 = \"data/m19_land_rating.tsv\"\n",
    "\n",
    "# Load data. Retry if parsing error. \n",
    "train_data, train_tensor, m19_set, le = ds.load_dataset(rating_path1, rating_path2, \"data/full_dataset/train.csv\")\n",
    "# val_data, val_tensor, m19_set, le = ds.load_dataset(rating_path1, rating_path2, \"data/full_dataset/test.csv\")\n",
    "\n",
    "\n",
    "#train_data, train_tensor, m19_set, le = ds.load_dataset(rating_path1, rating_path2, \"data/subset20000/train.csv\")\n",
    "#val_data, val_tensor, m19_set, le = ds.load_dataset(rating_path1, rating_path2, \"data/subset20000/val.csv\")\n",
    "#test_data, test_tensor, m19_set, le = load_dataset(rating_path1, rating_path2, \"data/subset20000/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing draft: 0.\n",
      "Processing draft: 10000.\n",
      "Processing draft: 20000.\n"
     ]
    }
   ],
   "source": [
    "val_data, val_tensor, m19_set, le = ds.load_dataset(rating_path1, rating_path2, \"data/full_dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set_vector(casting_cost, card_type, rarity, color_vector):\n",
    "    \"\"\"\n",
    "    Returns a one hot encoded card property vector. \n",
    "    \n",
    "    There are 21 binary features:\n",
    "    \n",
    "    0. cmc=0\n",
    "    1. cmc=1\n",
    "    2. cmc=2\n",
    "    3. cmc=3\n",
    "    4. cmc=4\n",
    "    5. cmc=5\n",
    "    6. cmc=6\n",
    "    7. cmc>=7\n",
    "    8. creature?\n",
    "    9. common?\n",
    "    10. uncommon?\n",
    "    11. rare?\n",
    "    12. mythic?\n",
    "    13. colorless?\n",
    "    14. monocolored?\n",
    "    15. multicolored?\n",
    "    16. color1?\n",
    "    17. color2?\n",
    "    18. color3?\n",
    "    19. color4?\n",
    "    20. color5?\n",
    "    \n",
    "    :param casting_cost: integer casting cost of card\n",
    "    :param card_type: \"Creature\" or other\n",
    "    :param rarity\": \"C\", \"U\", \"R\", or \"M\"\n",
    "    \"param color_vector\": vector corresponding to colors of card, example: [1,0,0,0,1]\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialize set vector.\n",
    "    v = [0] * 21\n",
    "    \n",
    "    # Encode cmc. \n",
    "    if casting_cost == 0:\n",
    "        v[0] = 1\n",
    "    elif casting_cost == 1:\n",
    "        v[1] = 1\n",
    "    elif casting_cost == 2:\n",
    "        v[2] = 1\n",
    "    elif casting_cost == 3:\n",
    "        v[3] = 1\n",
    "    elif casting_cost == 4:\n",
    "        v[4] = 1\n",
    "    elif casting_cost == 5:\n",
    "        v[5] = 1\n",
    "    elif casting_cost == 6:\n",
    "        v[6] = 1\n",
    "    elif casting_cost >= 7:\n",
    "        v[7] = 1\n",
    "    else:\n",
    "        print(\"WARNING: Undefined casting cost.\")\n",
    "    \n",
    "    # Encode type.\n",
    "    if card_type == \"Creature\":\n",
    "        v[8] = 1\n",
    "        \n",
    "    # Encode rarity.\n",
    "    if rarity == \"C\":\n",
    "        v[9] = 1\n",
    "    elif rarity == \"U\":\n",
    "        v[10] = 1\n",
    "    elif rarity == \"R\":\n",
    "        v[11] = 1\n",
    "    elif rarity == \"M\":\n",
    "        v[12] = 1\n",
    "    \n",
    "    # Process number of colors.\n",
    "    num_colors = len([c for c in color_vector if c > 0])\n",
    "    if num_colors == 0:\n",
    "        v[13] = 1\n",
    "    elif num_colors == 1:\n",
    "        v[14] = 1\n",
    "    elif num_colors >= 2:\n",
    "        v[15] = 1\n",
    "    \n",
    "    # Process card color. \n",
    "    if color_vector[0] > 0:\n",
    "        v[16] = 1\n",
    "    if color_vector[1] > 0:\n",
    "        v[17] = 1\n",
    "    if color_vector[2] > 0:\n",
    "        v[18] = 1\n",
    "    if color_vector[3] > 0:\n",
    "        v[19] = 1\n",
    "    if color_vector[4] > 0:\n",
    "        v[20] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmc_from_string(cmc_string):\n",
    "    \"\"\"\n",
    "    Return an integer converted mana cost from cmc_string. \n",
    "    \n",
    "    Each character adds 1 to cmc. \n",
    "    \n",
    "    :param cmc_string: String or integer representation of cmc. Example: \"1UBR\".\n",
    "    :returns: Integer cmc. Example: 4.\n",
    "    \"\"\"\n",
    "    # If int, we are done. \n",
    "    if type(cmc_string) is int:\n",
    "        return cmc_string\n",
    "    \n",
    "    # Convert string to integer cmc.\n",
    "    cmc = 0\n",
    "    digit_string = \"\"\n",
    "    letters = set(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "    digits = set(\"1234567890\")\n",
    "        \n",
    "    for c in cmc_string:        \n",
    "        if c in letters:\n",
    "            cmc += 1\n",
    "        else:\n",
    "            digit_string += c\n",
    "    if len(digit_string) > 0:\n",
    "        cmc += int(digit_string)\n",
    "    return cmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set_tensor(magic_set):\n",
    "    \"\"\"\n",
    "    Returns a set tensor which represents the properties of cards in the set.\n",
    "    \n",
    "    There are M features and N cards in the set and the tensor is of size M x N.\n",
    "    \n",
    "    The features are documented in the create_set_vector() function. \n",
    "    \"\"\"\n",
    "    set_list = []\n",
    "    \n",
    "    # Requires these names to be present in the set file.\n",
    "    reduced_set = magic_set[[\"Name\", \"Casting Cost 1\", \"Card Type\", \"Rarity\", \"Color Vector\"]]\n",
    "    for index, row in reduced_set.iterrows():\n",
    "        card_vector = create_set_vector(cmc_from_string(row[1]), row[2], row[3], row[4])\n",
    "        set_list.append(card_vector)\n",
    "        \n",
    "    # set_list is currently N x M list of lists. \n",
    "    set_flipped = torch.Tensor(set_list)\n",
    "    set_tensor = torch.transpose(set_flipped, 0, 1)\n",
    "    return set_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21, 285])\n"
     ]
    }
   ],
   "source": [
    "# Set tensor.\n",
    "st = create_set_tensor(m19_set)\n",
    "if device.type != \"cpu\":\n",
    "    st = st.cuda()\n",
    "print(st.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement NN.\n",
    "class DraftNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, set_tensor):\n",
    "        \"\"\"Placeholder NN. Currently does nothing.\n",
    "        \n",
    "        param ss: number of cards in set\n",
    "        param set_tensor: Mxss set tensor describing the set\n",
    "        \"\"\"\n",
    "        super(DraftNet, self).__init__()\n",
    "        \n",
    "        # Load set tensor.\n",
    "        self.set_tensor = set_tensor\n",
    "        self.set_tensor_tranpose = torch.transpose(set_tensor, 0, 1)\n",
    "        self.M, self.ss = self.set_tensor.shape\n",
    "        self.half_ss = self.ss / 2\n",
    "        \n",
    "        # Specify layer sizes. \n",
    "        size_in = self.ss + self.M\n",
    "        #size_in = self.ss\n",
    "        size1 = self.ss\n",
    "        size2 = self.ss\n",
    "        size3 = self.ss\n",
    "        size4 = self.ss\n",
    "        size5 = self.ss\n",
    "        size6 = self.ss\n",
    "        size7 = self.ss\n",
    "        size8 = self.ss\n",
    "        \n",
    "        self.ns = 0.01\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(self.ss + self.M)\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(size_in, size1)\n",
    "        self.bn1 = nn.BatchNorm1d(size1)\n",
    "        self.relu1 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear2 = torch.nn.Linear(size1, size2)\n",
    "        self.bn2 = nn.BatchNorm1d(size2)\n",
    "        self.relu2 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear3 = torch.nn.Linear(size2, size3)\n",
    "        self.bn3 = nn.BatchNorm1d(size3)\n",
    "        self.relu3 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear4 = torch.nn.Linear(size3, size4)\n",
    "        self.relu4 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear5 = torch.nn.Linear(size3, size5)\n",
    "        self.relu5 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear6 = torch.nn.Linear(size3, size6)\n",
    "        self.relu6 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        self.dropout6 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear7 = torch.nn.Linear(size3, size7)\n",
    "        self.relu7 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        self.dropout7 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear8 = torch.nn.Linear(size3, size8)\n",
    "        self.relu8 = torch.nn.LeakyReLU(negative_slope = self.ns)\n",
    "        \n",
    "        \n",
    "        #self.sm = torch.nn.Softmax()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        \n",
    "        collection = x[:, :self.ss]\n",
    "        \n",
    "        #collection = self.bn(collection)\n",
    "        \n",
    "        pack = x[:, self.ss:]\n",
    "        \n",
    "        # Get features from set tensor. \n",
    "        features = torch.mm(collection, self.set_tensor_tranpose)\n",
    "        collection_and_features = torch.cat((collection, features), 1)\n",
    "        \n",
    "        collection_and_features = self.bn(collection_and_features)\n",
    "        \n",
    "        #y = self.linear1(collection_and_features)\n",
    "        y = self.linear1(collection_and_features)\n",
    "        y = self.bn1(y)\n",
    "        y = self.relu1(y)\n",
    "        y = self.dropout1(y)\n",
    "        \n",
    "        y = self.linear2(y)\n",
    "        y = self.bn2(y)\n",
    "        y = self.relu2(y)\n",
    "        y = self.dropout2(y)\n",
    "        \n",
    "        y = self.linear3(y)\n",
    "        y = self.bn3(y)\n",
    "        y = self.relu3(y)\n",
    "        y = self.dropout3(y)\n",
    "\n",
    "        y = self.linear4(y)\n",
    "        #y = self.relu4(y)\n",
    "        #y = self.dropout4(y)\n",
    "        \n",
    "        #y = self.linear5(y)\n",
    "        #y = self.relu5(y)\n",
    "        #y = self.dropout5(y)\n",
    "        \n",
    "        #y = self.linear6(y)\n",
    "        #y = self.relu6(y)\n",
    "        #y = self.dropout6(y)\n",
    "        \n",
    "        #y = self.linear7(y)\n",
    "        #y = self.relu7(y)\n",
    "        #y = self.dropout7(y)\n",
    "        \n",
    "        #y = self.linear8(y)\n",
    "        #y = self.relu8(y)\n",
    "        \n",
    "        y = y * pack # Enforce cards in pack only.        \n",
    "        return y\n",
    "\n",
    "#Create NN.\n",
    "net = DraftNet(st)\n",
    "\n",
    "if device.type != \"cpu\":\n",
    "    net = net.cuda()\n",
    "\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net, dataloader, num_epoch, optimizer):\n",
    "    \"\"\"Train the network.\"\"\"\n",
    "    net.train()    \n",
    "    my_count = 0\n",
    "    for epoch in range(num_epoch):\n",
    "        \n",
    "        #Loop over x,y for each dataset.\n",
    "        running_loss = 0\n",
    "        for i, data in enumerate(dataloader):\n",
    "        \n",
    "            my_count+=1\n",
    "            if my_count % 10000 == 0:\n",
    "                print(my_count)\n",
    "        \n",
    "            #Get the inputs. Keeps batch size.\n",
    "            x, y = data\n",
    "            \n",
    "            # cuda() is needed for GPU mode. Not sure why.\n",
    "            if device.type != \"cpu\":\n",
    "                x = x.cuda()\n",
    "                y = y.cuda() # One-hot encoded. \n",
    "            \n",
    "            # Zero parameter gradients between batches.\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            #Perform training.\n",
    "            y_pred = net(x)\n",
    "            y_integer = torch.argmax(y, 1) # Class indices.\n",
    "            \n",
    "            # Use cross entropy loss. \n",
    "            loss = torch.nn.CrossEntropyLoss()\n",
    "            output = loss(y_pred, y_integer)\n",
    "            output.backward()\n",
    "            optimizer.step()\n",
    "                        \n",
    "            #Print loss data.\n",
    "            running_loss += output.item()\n",
    "            step = 1\n",
    "            if i % len(dataloader) == len(dataloader)-1 and (epoch + 1) % step == 0:\n",
    "                print('Train Cross-Entropy Loss: %.6f' % (running_loss/len(dataloader)))\n",
    "                running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataloaders. \n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(val_data, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_net(net, dataloader):\n",
    "    \"\"\"Compute accuracy on validation set.\"\"\"\n",
    "    net.eval()\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "        \n",
    "            #Get the inputs. Keeps batch size.\n",
    "            x, y = data\n",
    "            \n",
    "            # cuda() is needed for GPU mode. Not sure why.\n",
    "            if device.type != \"cpu\":\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "            y_integer = torch.argmax(y, 1) # Class indices.\n",
    "            \n",
    "            # Compute val loss.\n",
    "            y_pred = net(x)\n",
    "            y_pred_integer = torch.argmax(y_pred, 1)\n",
    "            \n",
    "            # Compute accuracy. \n",
    "            correct += int(sum(y_pred_integer == y_integer))\n",
    "            total += len(y_integer)\n",
    "            \n",
    "    accuracy = correct / total\n",
    "\n",
    "    print(\"Validation accuracy:\", accuracy, \" Total picks:\", int(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload 0.636\n",
    "# net = torch.load(\"draftnet_aug_22_2019.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "Train Cross-Entropy Loss: 1.083126\n",
      "Validation accuracy: 0.6372559312438887  Total picks: 971550\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/envs/keras/lib/python3.6/site-packages/torch/serialization.py:250: UserWarning: Couldn't retrieve source code for container of type DraftNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "Train Cross-Entropy Loss: 1.016387\n",
      "Validation accuracy: 0.6416252380217179  Total picks: 971550\n",
      "Epoch: 3\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "Train Cross-Entropy Loss: 1.004292\n",
      "Validation accuracy: 0.6433359065410942  Total picks: 971550\n",
      "Epoch: 4\n",
      "10000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "# Now leave optimizer out here. \n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "#optimizer = optim.ASGD(net.parameters())\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.0)\n",
    "#optimizer = torch.optim.RMSprop(net.parameters(), lr=0.01, momentum=0.0)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.33) # Used for exp 47.\n",
    "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.97)\n",
    "\n",
    "ep = 0\n",
    "for run in range(25):\n",
    "    \n",
    "    ep += 1\n",
    "    print(\"Epoch:\", ep)\n",
    "    \n",
    "    # Train1.\n",
    "    train_net(net, trainloader, 1, optimizer)\n",
    "    #scheduler.step()\n",
    "    \n",
    "    # Validation. \n",
    "    val_net(net, valloader)\n",
    "    \n",
    "    torch.save(net, \"draftnet_jan19_2020_ep\" + str(ep)  + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Train the network over several epochs - no momentu, from scratch.\n",
    "# run = 53\n",
    "\n",
    "# # Now leave optimizer out here. \n",
    "# #optimizer = optim.Adam(net.parameters(), lr=0.0001, betas=(0.9, 0.999)) #Good\n",
    "# #optimizer = optim.ASGD(net.parameters())\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=0.00001, momentum=0.0)\n",
    "\n",
    "# #optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.0)\n",
    "# #optimizer = torch.optim.RMSprop(net.parameters(), lr=0.01, momentum=0.0)\n",
    "# # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.33) # Used for exp 47.\n",
    "# #scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.985)\n",
    "\n",
    "# ep = 0\n",
    "\n",
    "# for run in range(999999):\n",
    "    \n",
    "#     ep += 1\n",
    "#     print(\"Epoch:\", ep)\n",
    "    \n",
    "#     # Train1.\n",
    "#     train_net(net, trainloader, 1, optimizer)\n",
    "#     scheduler.step()\n",
    "    \n",
    "#     # Validation. \n",
    "#     val_net(net, valloader)\n",
    "    \n",
    "#     torch.save(net, \"draftnet_jan19_2020_ep\" + str(ep)  + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net, \"draftnet_jan_19_2020.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.SGD(net.parameters(), lr=0.00001, momentum=0.0)\n",
    "# # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.33) # Used for exp 47.\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.8)\n",
    "\n",
    "# ep = 0\n",
    "\n",
    "# for run in range(999999):\n",
    "    \n",
    "#     ep += 1\n",
    "#     print(\"Epoch:\", ep)\n",
    "    \n",
    "#     # Train1.\n",
    "#     train_net(net, trainloader, 1, optimizer)\n",
    "#     scheduler.step()\n",
    "    \n",
    "#     # Validation. \n",
    "#     val_net(net, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net, \"draftnet_oct_17_2019_633_cpu.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
